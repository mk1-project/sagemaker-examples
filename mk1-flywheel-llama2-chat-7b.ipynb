{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# Run Llama 2 7B in SageMaker using MK1 Flywheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b1b24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "MK1 is building the worldâ€™s most efficient generative AI platform to run foundations models in the cloud. We routinely save customers >50% in cloud costs, and offer the lowest latency for a given throughput. Take it for a spin, and contact us when you are ready to run it in production.\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy the MK1 Flywheel runtime using the Llama 2 fine-tuned model optimized for dialogue use cases.\n",
    "\n",
    "By using this example, you are agreeing to all terms and conditions described in the Llama2 end-user-license-agreement (EULA) (https://ai.meta.com/llama/license/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "In order to run the following example, the following pre-requisites need to be satisfied:\n",
    "1. The IAM role requires **AmazonSageMakerFullAccess** permissions and the `AssumeRole` trust relationship (see below).\n",
    "2. One of the following:\n",
    "    1. The IAM role has the necessary permissions to automatically subscribe to the corresponding AWS Marketplace listing.\n",
    "    2. The AWS account already has an active subscription.\n",
    "\n",
    "#### Trust Relationship for IAM role\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52fe51",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "try:\n",
    "    execution_role_arn = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    execution_role_arn = None\n",
    "\n",
    "if execution_role_arn == None:\n",
    "    execution_role_arn = input(\"Enter your execution role ARN: \")\n",
    "\n",
    "model_package_map = {\n",
    "    \"us-west-2\": \"arn:aws:sagemaker:us-west-2:123488637174:model-package/mk1-flywheel-v063-llama2-7b-chat-s0\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eef0dd",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "---\n",
    "\n",
    "You can now deploy the model using SageMaker. The following instance types are supported:\n",
    "- ml.g5.xlarge\n",
    "- ml.g5.2xlarge\n",
    "- ml.g5.4xlarge\n",
    "- ml.g5.8xlarge\n",
    "- ml.g5.16xlarge\n",
    "\n",
    "**NOTE:** Despite the fact that all of these instances have only one GPU, the CPU count is relevant when running high-throughput workloads to handle all the incoming connections.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = sagemaker.ModelPackage(\n",
    "    model_package_arn=model_package_arn,\n",
    "    role=execution_role_arn,\n",
    ")\n",
    "model.deploy(initial_instance_count=1, instance_type='ml.g5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "\n",
    "The example below shows the minimal code required to run inference from the endpoint.\n",
    "\n",
    "### Supported Parameters\n",
    "The plain model API supports the following inference parameters:\n",
    "\n",
    "- **text (str):** The initial input text provided to the model. This text serves as the context or prompt based on which the model generates additional content.\n",
    "- **max_tokens (int):** The maximum number of tokens that the model will generate in response to the input text. \n",
    "- **max_input_tokens (int, default 0):** This specifies the maximum number of tokens allowed in the input text. If the input text exceeds this number, it will be truncated.\n",
    "- **num_samples (int, default 1):** The number of independent completions to generate for the given input text. Each sample is generated separately and may result in different outputs.\n",
    "- **eos_token_ids (List[int], default [1, 2]):** A list of token IDs that signify the end of a sequence. When the model generates one of these tokens, it considers the output complete and stops generating further tokens.\n",
    "- **stop (List[str], default []):** A list of strings where, if the model generates them, it will stop further text generation. The stop string is not included in the returned output.\n",
    "- **temperature (float, default 1.0):** Controls the degree of determinism in the output. A higher temperature leads to more varied output, while a lower temperature makes the model more likely to choose high-probability logits. At a temperature of 0, the model performs greedy sampling.\n",
    "- **top_k (int, default 50):** This parameter narrows down the choice of next words to the top 'k' most likely options, based on the model's predictions. It helps in focusing the generation on more probable logits.\n",
    "- **top_p (float, range 0 to 1, default 1.0):** This parameter allows the model to choose from the smallest set of logits whose cumulative probability does not exceed 'p'. This can create more diverse and less predictable text compared to top_k.\n",
    "- **presence_penalty (float, range -2 to 2, default 0.0):** This parameter adjusts the likelihood of the model introducing new topics or entities during text generation. A positive value encourages the introduction of new concepts by reducing repetition.\n",
    "- **frequency_penalty (float, range -2 to 2, default 0.0):** This parameter alters the likelihood of the model repeating the same line of thought or specific words. Positive values discourage repetition, encouraging the model to introduce more varied language and ideas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d506fb-f8d1-4870-a767-02c725562d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the difference between a Llama and an Alpaca?\"\n",
    "\n",
    "payload = {\n",
    "    'text': prompt,\n",
    "    'max_tokens': 500\n",
    "}\n",
    "\n",
    "response = model.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(\n",
    "            EndpointName=model.endpoint_name,\n",
    "            Body=bytes(json.dumps(payload), 'utf-8'),\n",
    "            ContentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "response = json.loads(response['Body'].read().decode('utf-8'))\n",
    "\n",
    "print(prompt)\n",
    "print(response[\"responses\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927c30f-ee07-459e-ae64-95d213d0fda4",
   "metadata": {},
   "source": [
    "## Run examples\n",
    "\n",
    "---\n",
    "\n",
    "The following examples have been adapted to match the input format used by the Llama2 JumpStart endpoints.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411deac-14c1-4b49-9094-9a904a3daff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions for JumpStart Llama2 endpoint.\n",
    "\n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and\n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "\n",
    "    T_BSYS, T_ESYS = \"<<SYS>>\", \"<</SYS>>\\n\"\n",
    "    T_BINST, T_EINST = \"[INST]\", \"[/INST]\\n\"\n",
    "    T_BOS, T_EOS = \"<s>\", \"</s>\\n\"\n",
    "\n",
    "    prompt: List[str] = []\n",
    "\n",
    "    if instructions[0][\"role\"] == \"system\":\n",
    "        prompt.extend([T_BSYS, instructions[0][\"content\"], T_ESYS])\n",
    "        instructions = instructions[1:]\n",
    "\n",
    "    for user, answer in zip(instructions[0::2], instructions[1::2]):\n",
    "        prompt.extend([T_BOS, T_BINST, (user[\"content\"]).strip(), T_EINST, (answer[\"content\"]).strip(), T_EOS])\n",
    "\n",
    "    prompt.extend([T_BOS, T_BINST, (instructions[-1][\"content\"]).strip(), T_EINST])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "def predict(payload):\n",
    "    model_payload = {\n",
    "        'text': format_instructions(payload[\"inputs\"][0]),\n",
    "    }\n",
    "    model_payload.update(payload[\"parameters\"])\n",
    "\n",
    "    response = model.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(\n",
    "            EndpointName=model.endpoint_name,\n",
    "            Body=bytes(json.dumps(model_payload), 'utf-8'),\n",
    "            ContentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "    return json.loads(response['Body'].read().decode('utf-8'))\n",
    "\n",
    "\n",
    "def print_dialog(payload, response):\n",
    "    dialog = payload[\"inputs\"][0]\n",
    "    for msg in dialog:\n",
    "        print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n",
    "    print(f\"> ASSISTANT: {response['responses'][0]['text']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbb9af",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbde5e7-1068-41f9-999a-70ef04e1cbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"user\", \"content\": \"What is the recipe of mayonnaise?\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predict(payload)\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574e4e2",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda81ccf-0188-4117-8355-801ef98aaa48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\"\\\n",
    "Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
    "\n",
    "1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
    "2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
    "3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
    "\n",
    "These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is so great about #1?\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predict(payload)\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8d152",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e8250-88c8-4b1c-a70b-ae5a4976e6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n",
    "        {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predict(payload)\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076644d4",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da83b4a-1e61-495c-b509-38266f5c44eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Always answer with emojis\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"How to go from Beijing to NY?\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predict(payload)\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "model.sagemaker_session.delete_endpoint(model.endpoint_name)\n",
    "model.sagemaker_session.delete_model(model.name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
